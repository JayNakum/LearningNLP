{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA1 = '''Formula One (more commonly known as Formula 1 or F1) is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the F�d�ration Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950. The word formula in the name refers to the set of rules to which all participants' cars must conform. A Formula One season consists of a series of races, known as Grands Prix. Grands Prix take place in multiple countries and continents around the world on either purpose - built circuits or closed public roads.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Formula', 'One', '(', 'more', 'commonly', 'known', 'as', 'Formula', '1', 'or', 'F1', ')', 'is', 'the', 'highest', 'class', 'of', 'international', 'racing', 'for', 'open-wheel', 'single-seater', 'formula', 'racing', 'cars', 'sanctioned', 'by', 'the', 'F�d�ration', 'Internationale', 'de', \"l'Automobile\", '(', 'FIA', ')', '.', 'The', 'FIA', 'Formula', 'One', 'World', 'Championship', 'has', 'been', 'one', 'of', 'the', 'premier', 'forms', 'of', 'racing', 'around', 'the', 'world', 'since', 'its', 'inaugural', 'season', 'in', '1950', '.', 'The', 'word', 'formula', 'in', 'the', 'name', 'refers', 'to', 'the', 'set', 'of', 'rules', 'to', 'which', 'all', 'participants', \"'\", 'cars', 'must', 'conform', '.', 'A', 'Formula', 'One', 'season', 'consists', 'of', 'a', 'series', 'of', 'races', ',', 'known', 'as', 'Grands', 'Prix', '.', 'Grands', 'Prix', 'take', 'place', 'in', 'multiple', 'countries', 'and', 'continents', 'around', 'the', 'world', 'on', 'either', 'purpose', '-', 'built', 'circuits', 'or', 'closed', 'public', 'roads', '.']\n",
      "[\"Formula One (more commonly known as Formula 1 or F1) is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the F�d�ration Internationale de l'Automobile (FIA).\", 'The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950.', \"The word formula in the name refers to the set of rules to which all participants' cars must conform.\", 'A Formula One season consists of a series of races, known as Grands Prix.', 'Grands Prix take place in multiple countries and continents around the world on either purpose - built circuits or closed public roads.']\n",
      "['Formula', 'One', '(more', 'commonly', 'known', 'as', 'Formula', '1', 'or', 'F1)', 'is', 'the', 'highest', 'class', 'of', 'international', 'racing', 'for', 'open-wheel', 'single-seater', 'formula', 'racing', 'cars', 'sanctioned', 'by', 'the', 'F�d�ration', 'Internationale', 'de', \"l'Automobile\", '(FIA).', 'The', 'FIA', 'Formula', 'One', 'World', 'Championship', 'has', 'been', 'one', 'of', 'the', 'premier', 'forms', 'of', 'racing', 'around', 'the', 'world', 'since', 'its', 'inaugural', 'season', 'in', '1950.', 'The', 'word', 'formula', 'in', 'the', 'name', 'refers', 'to', 'the', 'set', 'of', 'rules', 'to', 'which', 'all', \"participants'\", 'cars', 'must', 'conform.', 'A', 'Formula', 'One', 'season', 'consists', 'of', 'a', 'series', 'of', 'races,', 'known', 'as', 'Grands', 'Prix.', 'Grands', 'Prix', 'take', 'place', 'in', 'multiple', 'countries', 'and', 'continents', 'around', 'the', 'world', 'on', 'either', 'purpose', '-', 'built', 'circuits', 'or', 'closed', 'public', 'roads.']\n"
     ]
    }
   ],
   "source": [
    "# using nltk\n",
    "print(nltk.word_tokenize(DATA1))\n",
    "print(nltk.sent_tokenize(DATA1))\n",
    "print(nltk.WhitespaceTokenizer().tokenize(DATA1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(data:str) -> list:\n",
    "  tokens = []\n",
    "  token = ''\n",
    "  for chr in data:\n",
    "    if (chr in [' ', '(', ')', '[', ']', '.']):\n",
    "      if (len(token) > 0): tokens.append(token)\n",
    "      token = ''\n",
    "    else:\n",
    "      token += chr\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenizer(data:str, max_thresh = 3) -> list:\n",
    "  tokens = []\n",
    "  token = ''\n",
    "  for chr in data:\n",
    "    token += chr\n",
    "    if (chr == '\\n'):\n",
    "      tokens.append(token)\n",
    "      token = ''\n",
    "    if (chr == '.'):\n",
    "      if (len(word_tokenizer(token).pop()) > max_thresh):\n",
    "        tokens.append(token)\n",
    "        token = ''\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Formula', 'One', 'more', 'commonly', 'known', 'as', 'Formula', '1', 'or', 'F1', 'is', 'the', 'highest', 'class', 'of', 'international', 'racing', 'for', 'open-wheel', 'single-seater', 'formula', 'racing', 'cars', 'sanctioned', 'by', 'the', 'F�d�ration', 'Internationale', 'de', \"l'Automobile\", 'FIA', 'The', 'FIA', 'Formula', 'One', 'World', 'Championship', 'has', 'been', 'one', 'of', 'the', 'premier', 'forms', 'of', 'racing', 'around', 'the', 'world', 'since', 'its', 'inaugural', 'season', 'in', '1950', 'The', 'word', 'formula', 'in', 'the', 'name', 'refers', 'to', 'the', 'set', 'of', 'rules', 'to', 'which', 'all', \"participants'\", 'cars', 'must', 'conform', 'A', 'Formula', 'One', 'season', 'consists', 'of', 'a', 'series', 'of', 'races,', 'known', 'as', 'Grands', 'Prix', 'Grands', 'Prix', 'take', 'place', 'in', 'multiple', 'countries', 'and', 'continents', 'around', 'the', 'world', 'on', 'either', 'purpose', '-', 'built', 'circuits', 'or', 'closed', 'public', 'roads']\n",
      "[\"Formula One (more commonly known as Formula 1 or F1) is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the F�d�ration Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950.\", \" The word formula in the name refers to the set of rules to which all participants' cars must conform.\", ' A Formula One season consists of a series of races, known as Grands Prix.', ' Grands Prix take place in multiple countries and continents around the world on either purpose - built circuits or closed public roads.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenizer(DATA1))\n",
    "print(sent_tokenizer(DATA1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vowel(letter:str) -> bool:\n",
    "  if (letter in ['a', 'e', 'i', 'o', 'u']):\n",
    "    return True\n",
    "  if (letter == 'y'):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def is_consonant(letter:str) -> bool:\n",
    "  return not is_vowel(letter)\n",
    "\n",
    "def x_V_x(stem:str) -> bool:\n",
    "  for letter in stem:\n",
    "    if (is_vowel(letter)):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "def x_D(stem:str) -> bool:\n",
    "  l1 = stem[-1]\n",
    "  l2 = stem[-2]\n",
    "  return is_consonant(l1) and is_consonant(l2)\n",
    "\n",
    "def x_O(stem:str) -> bool:\n",
    "  if (len(stem) < 3): return False\n",
    "\n",
    "  c1 = stem[-3]\n",
    "  v = stem[-2]\n",
    "  c2 = stem[-1]\n",
    "\n",
    "  if (c2 in ['w', 'x', 'y']):\n",
    "    return False\n",
    "  \n",
    "  return is_consonant(c1) and is_vowel(v) and is_consonant(c2)\n",
    "\n",
    "def get_form(stem:str) -> str:\n",
    "  form = ' '\n",
    "\n",
    "  for letter in stem:\n",
    "    if (is_vowel(letter)):\n",
    "      if (form[-1] != 'v'):\n",
    "        form += 'v'\n",
    "    else:\n",
    "      if (form[-1] != 'c'):\n",
    "        form += 'c'\n",
    "\n",
    "  return form\n",
    "\n",
    "def get_m(stem:str) -> int:\n",
    "  form = get_form(stem)\n",
    "  return form.count('vc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(stem:str) -> str:\n",
    "  if (stem.endswith('sses')):\n",
    "    return stem.replace('sses', 'ss')\n",
    "  if (stem.endswith('ies')):\n",
    "    return stem.replace('ies', 'i')\n",
    "  if (stem.endswith('ss')):\n",
    "    # return stem.replace('ss', 'ss')\n",
    "    return stem\n",
    "  if (stem.endswith('s')):\n",
    "    # return stem.replace('s', '')\n",
    "    return stem[0:-1]\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2b(stem:str) -> str:\n",
    "  if (stem.endswith('at')):\n",
    "    return stem + 'e'\n",
    "  if (stem.endswith('bl')):\n",
    "    return stem + 'e'\n",
    "  if (x_O(stem) and not (stem.endswith(('l', 's', 'z')))):\n",
    "    return stem[0:-1]\n",
    "  if (get_m(stem) > 1 and x_O(stem)):\n",
    "    return stem + 'e'\n",
    "  return stem\n",
    "\n",
    "def step2(stem:str) -> str:\n",
    "  if (get_m(stem) > 1 and stem.endswith('eed')):\n",
    "    return stem.replace('eed', 'ee')\n",
    "  if (x_V_x(stem) and stem.endswith('ed')):\n",
    "    # return step2b(stem.replace('ed', ''))\n",
    "    return step2b(stem[0:-2])\n",
    "  if (x_V_x(stem) and stem.endswith('ing')):\n",
    "    # return step2b(stem.replace('ing', ''))\n",
    "    return step2b(stem[0:-3])\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(stem:str) -> str:\n",
    "  if (x_V_x(stem) and stem.endswith('y')):\n",
    "    stem[-1] = 'i'\n",
    "    return stem\n",
    "    # return stem.replace('y', 'i')\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(stem:str) -> str:\n",
    "  if (get_m(stem) > 0):\n",
    "    if (stem.endswith('ational')):\n",
    "      return stem.replace('ational', 'ate')\n",
    "    if (stem.endswith('ization')):\n",
    "      return stem.replace('ization', 'ize')\n",
    "    if (stem.endswith('biliti')):\n",
    "      return stem.replace('biliti', 'ble')\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step5(stem:str) -> str:\n",
    "  if (get_m(stem) > 0):\n",
    "    if (stem.endswith('icate')):\n",
    "      return stem.replace('icate', 'ic')\n",
    "    if (stem.endswith('ful')):\n",
    "      return stem[0:-3]\n",
    "    if (stem.endswith('ness')):\n",
    "      return stem[0:-4]\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step6(stem:str) -> str:\n",
    "  if (get_m(stem) > 0):\n",
    "    if (stem.endswith('ance')):\n",
    "      return stem.replace('ance', '')\n",
    "    if (stem.endswith('ent')):\n",
    "      return stem.replace('ent', '')\n",
    "    if (stem.endswith('ive')):\n",
    "      return stem.replace('ive', '')\n",
    "    if (stem.endswith('ize')):\n",
    "      return stem.replace('ize', '')\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step7a(stem:str) -> str:\n",
    "  if (get_m(stem) > 1 and stem.endswith('e')):\n",
    "    return stem[0:-1]\n",
    "  if (((get_m(stem) == 1) and not x_O(stem)) and stem.endswith('ness')):\n",
    "    return stem.replace('ness', '')\n",
    "    # return stem[0:-4]\n",
    "  return stem\n",
    "  \n",
    "def step7b(stem:str) -> str:\n",
    "  if ((get_m(stem) > 1) and x_D(stem) and stem.endswith('l')):\n",
    "    return stem[0:-1]\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PorterStemmer(word:str) -> str:\n",
    "  stem = word\n",
    "  stem = step1(stem)\n",
    "  stem = step2(stem)\n",
    "  stem = step3(stem)\n",
    "  stem = step4(stem)\n",
    "  stem = step5(stem)\n",
    "  stem = step6(stem)\n",
    "  stem = step7a(stem)\n",
    "  stem = step7b(stem)\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "sing\n",
      "control\n",
      "general\n",
      "elephant\n",
      "do\n"
     ]
    }
   ],
   "source": [
    "for word in ['computers', 'singing', 'controlling', 'generalizations', 'elephants', 'doing']:\n",
    "  print(PorterStemmer(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites: kite\n",
      "babies: baby\n",
      "dogs: dog\n",
      "flying: flying\n",
      "smiling: smiling\n",
      "driving: driving\n",
      "died: died\n",
      "tried: tried\n",
      "feet: foot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
    "for word in words:\n",
    "  print(word + \": \" + lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "  word = word.lower()\n",
    "\n",
    "  # Handling irregular nouns\n",
    "  irregular_nouns = {\n",
    "    'men': 'man',\n",
    "    'women': 'woman',\n",
    "    'children': 'child',\n",
    "    'teeth': 'tooth',\n",
    "    'feet': 'foot',\n",
    "    'mice': 'mouse'\n",
    "  }\n",
    "  if word in irregular_nouns:\n",
    "    return irregular_nouns[word]\n",
    "  \n",
    "  # Handling regular plural nouns\n",
    "  if word.endswith('ies'):\n",
    "    if len(word) > 3:\n",
    "      return word[:-3] + 'y'\n",
    "  elif word.endswith('es'):\n",
    "    if word[-3] in \"s,x,z,o\":\n",
    "      return word[:-2]\n",
    "    else:\n",
    "      return word[:-1]\n",
    "  elif word.endswith('s'):\n",
    "    return word[:-1]\n",
    "  \n",
    "  # Handling past tense and past participle of regular verbs\n",
    "  if word.endswith('ed'):\n",
    "    if len(word) > 2:\n",
    "      if word[-3] == word[-4]:  # e.g., stopped -> stop\n",
    "        return word[:-3]\n",
    "      return word[:-2]\n",
    "    \n",
    "  # Handling present participle of regular verbs\n",
    "  if word.endswith('ing'):\n",
    "    if len(word) > 3:\n",
    "      if word[-4] == word[-5]:  # e.g., running -> run\n",
    "        return word[:-4]\n",
    "      return word[:-3]\n",
    "  # Return the word if no rules are applied\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'stop', 'child', 'box', 'tooth']\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"stopped\", \"children\", \"boxes\", \"teeth\"]\n",
    "lemmatized_words = [lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: HMM Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/jaynakum/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jaynakum/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "print(nltk_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80310\n",
      "20366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Drink', 'NOUN'),\n",
       " ('Carrier', 'NOUN'),\n",
       " ('Competes', 'VERB'),\n",
       " ('With', 'ADP'),\n",
       " ('Cartons', 'NOUN')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_set,test_set = train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "\n",
    "# create list of train and test tagged words\n",
    "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
    "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))\n",
    "train_tagged_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'PRT', 'X', 'ADV', 'ADJ', 'ADP', 'NOUN', 'NUM', 'CONJ', 'VERB', 'DET', '.', 'PRON'}\n"
     ]
    }
   ],
   "source": [
    "# check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "print(len(tags))\n",
    "print(tags)\n",
    "\n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "  tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "  count_tag = len(tag_list) # total number of times the passed tag occurred in train_bag\n",
    "  w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "# now calculate the total number of times the passed word occurred as the passed tag.\n",
    "  count_w_given_tag = len(w_given_tag_list)\n",
    "  return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "  tags = [pair[1] for pair in train_bag]\n",
    "  count_t1 = len([t for t in tags if t==t1])\n",
    "  count_t2_t1 = 0\n",
    "  for index in range(len(tags)-1):\n",
    "    if tags[index]==t1 and tags[index+1] == t2:\n",
    "      count_t2_t1 += 1\n",
    "  return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRT</th>\n",
       "      <th>X</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>DET</th>\n",
       "      <th>.</th>\n",
       "      <th>PRON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.082975</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>0.250489</td>\n",
       "      <td>0.056751</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.401174</td>\n",
       "      <td>0.101370</td>\n",
       "      <td>0.045010</td>\n",
       "      <td>0.017613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.185086</td>\n",
       "      <td>0.075726</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>0.142226</td>\n",
       "      <td>0.061695</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.010379</td>\n",
       "      <td>0.206419</td>\n",
       "      <td>0.056890</td>\n",
       "      <td>0.160869</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.022886</td>\n",
       "      <td>0.081458</td>\n",
       "      <td>0.130721</td>\n",
       "      <td>0.119472</td>\n",
       "      <td>0.032196</td>\n",
       "      <td>0.029868</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.339022</td>\n",
       "      <td>0.071373</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>0.012025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.063301</td>\n",
       "      <td>0.080583</td>\n",
       "      <td>0.696893</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.066019</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.107062</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.323589</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.320931</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.069603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.043935</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.016895</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.176827</td>\n",
       "      <td>0.262344</td>\n",
       "      <td>0.009144</td>\n",
       "      <td>0.042454</td>\n",
       "      <td>0.149134</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.240094</td>\n",
       "      <td>0.004659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.026062</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.351660</td>\n",
       "      <td>0.184220</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>0.020707</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.119243</td>\n",
       "      <td>0.001428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.057080</td>\n",
       "      <td>0.113611</td>\n",
       "      <td>0.055982</td>\n",
       "      <td>0.349067</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.150384</td>\n",
       "      <td>0.123491</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.060373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.030663</td>\n",
       "      <td>0.215930</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.066390</td>\n",
       "      <td>0.092357</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.022836</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>0.133610</td>\n",
       "      <td>0.034807</td>\n",
       "      <td>0.035543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.045134</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>0.206411</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>0.635906</td>\n",
       "      <td>0.022855</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.040247</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.003306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.052569</td>\n",
       "      <td>0.046132</td>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.218539</td>\n",
       "      <td>0.078210</td>\n",
       "      <td>0.060079</td>\n",
       "      <td>0.089690</td>\n",
       "      <td>0.172192</td>\n",
       "      <td>0.092372</td>\n",
       "      <td>0.068769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.014123</td>\n",
       "      <td>0.088383</td>\n",
       "      <td>0.036902</td>\n",
       "      <td>0.070615</td>\n",
       "      <td>0.022323</td>\n",
       "      <td>0.212756</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.484738</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.041913</td>\n",
       "      <td>0.006834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PRT         X       ADV       ADJ       ADP      NOUN       NUM  \\\n",
       "PRT   0.001174  0.012133  0.009393  0.082975  0.019569  0.250489  0.056751   \n",
       "X     0.185086  0.075726  0.025754  0.017682  0.142226  0.061695  0.003075   \n",
       "ADV   0.014740  0.022886  0.081458  0.130721  0.119472  0.032196  0.029868   \n",
       "ADJ   0.011456  0.020971  0.005243  0.063301  0.080583  0.696893  0.021748   \n",
       "ADP   0.001266  0.034548  0.014553  0.107062  0.016958  0.323589  0.063275   \n",
       "NOUN  0.043935  0.028825  0.016895  0.012584  0.176827  0.262344  0.009144   \n",
       "NUM   0.026062  0.202428  0.003570  0.035345  0.037487  0.351660  0.184220   \n",
       "CONJ  0.004391  0.009330  0.057080  0.113611  0.055982  0.349067  0.040615   \n",
       "VERB  0.030663  0.215930  0.083886  0.066390  0.092357  0.110589  0.022836   \n",
       "DET   0.000287  0.045134  0.012074  0.206411  0.009918  0.635906  0.022855   \n",
       ".     0.002789  0.025641  0.052569  0.046132  0.092908  0.218539  0.078210   \n",
       "PRON  0.014123  0.088383  0.036902  0.070615  0.022323  0.212756  0.006834   \n",
       "\n",
       "          CONJ      VERB       DET         .      PRON  \n",
       "PRT   0.002348  0.401174  0.101370  0.045010  0.017613  \n",
       "X     0.010379  0.206419  0.056890  0.160869  0.054200  \n",
       "ADV   0.006982  0.339022  0.071373  0.139255  0.012025  \n",
       "ADJ   0.016893  0.011456  0.005243  0.066019  0.000194  \n",
       "ADP   0.001012  0.008479  0.320931  0.038724  0.069603  \n",
       "NOUN  0.042454  0.149134  0.013106  0.240094  0.004659  \n",
       "NUM   0.014281  0.020707  0.003570  0.119243  0.001428  \n",
       "CONJ  0.000549  0.150384  0.123491  0.035126  0.060373  \n",
       "VERB  0.005433  0.167956  0.133610  0.034807  0.035543  \n",
       "DET   0.000431  0.040247  0.006037  0.017393  0.003306  \n",
       ".     0.060079  0.089690  0.172192  0.092372  0.068769  \n",
       "PRON  0.005011  0.484738  0.009567  0.041913  0.006834  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# creating t x t transition matrix of tags, t= no of tags\n",
    "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "  for j, t2 in enumerate(list(tags)):\n",
    "    tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "\n",
    "# convert the matrix to a df for better readability\n",
    "#the table is same as the transition table shown in section 3 of article\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "display(tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "  state = []\n",
    "  T = list(set([pair[1] for pair in train_bag]))\n",
    "  for key, word in enumerate(words):\n",
    "    #initialise list of probability column for a given observation\n",
    "    p = []\n",
    "    for tag in T:\n",
    "      if key == 0:\n",
    "        transition_p = tags_df.loc['.', tag]\n",
    "      else:\n",
    "        transition_p = tags_df.loc[state[-1], tag]\n",
    "      # compute emission and state probabilities\n",
    "      emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "      state_probability = emission_p * transition_p\n",
    "      p.append(state_probability)\n",
    "    pmax = max(p)\n",
    "    # getting state for which probability is maximum\n",
    "    state_max = T[p.index(pmax)]\n",
    "    state.append(state_max)\n",
    "  return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Will', 'PRT'), ('can', 'VERB'), ('see', 'VERB'), ('Marry', 'PRT')]\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"Will can see Marry\"\n",
    "pred_tags_withoutRules = Viterbi(test_sent.split())\n",
    "print(pred_tags_withoutRules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Turney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadReviews(fileName):\n",
    "  list_pos = []\n",
    "  list_neg = []\n",
    "  data = []\n",
    "  with open(fileName, 'r') as f:\n",
    "    for line in f:\n",
    "      data.append(json.loads(line))\n",
    "  for elem in data:\n",
    "    if float(elem[\"overall\"]) >= 3.0:\n",
    "      list_pos.append(elem[\"reviewText\"])\n",
    "    else:\n",
    "      list_neg.append(elem[\"reviewText\"])\n",
    "  return list_pos, list_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(fileName):\n",
    "  all_positive_reviews, all_negative_reviews = loadReviews(fileName)\n",
    "  dataset = {'train': {'neg': [], 'pos': []}, 'test': {'neg': [], 'pos': []}}\n",
    "  dataset['train']['pos'] = (all_positive_reviews[:20000])\n",
    "  dataset['train']['neg'] = (all_negative_reviews[:20000])\n",
    "  dataset['test']['pos'] = (all_positive_reviews[-50:])\n",
    "  dataset['test']['neg'] = (all_negative_reviews[-50:])\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(postag):\n",
    "  tag_pattern = []\n",
    "  for k in range(len(postag) - 2):\n",
    "    if (postag[k][1] == \"JJ\" \n",
    "        and (postag[k + 1][1] == \"NN\" or postag[k + 1][1] == \"NNS\")):\n",
    "      tag_pattern.append(\"\".join(postag[k][0]) + \" \" + \"\".join(postag[k + 1][0]))\n",
    "\n",
    "    elif ((postag[k][1] == \"RB\" or postag[k][1] == \"RBR\" or postag[k][1] == \"RBS\") \n",
    "          and postag[k + 1][1] == \"JJ\" \n",
    "          and postag[k + 2][1] != \"NN\" and postag[k + 2][1] != \"NNS\"):\n",
    "      tag_pattern.append(\"\".join(postag[k][0]) + \" \" + \"\".join(postag[k + 1][0]))\n",
    "      \n",
    "    elif (postag[k][1] == \"JJ\" \n",
    "          and postag[k + 1][1] == \"JJ\" \n",
    "          and postag[k + 2][1] != \"NN\" and postag[k + 2][1] != \"NNS\"):\n",
    "      tag_pattern.append(\"\".join(postag[k][0]) + \" \" + \"\".join(postag[k + 1][0]))\n",
    "      \n",
    "    elif ((postag[k][1] == \"NN\" or postag[k][1] == \"NNS\") \n",
    "          and postag[k + 1][1] == \"JJ\" \n",
    "          and postag[k + 2][1] != \"NN\" and postag[k + 2][1] != \"NNS\"):\n",
    "      tag_pattern.append(\"\".join(postag[k][0]) + \" \" + \"\".join(postag[k + 1][0]))\n",
    "\n",
    "    elif ((postag[k][1] == \"RB\" or postag[k][1] == \"RBR\" or postag[k][1] == \"RBS\") \n",
    "          and (postag[k + 1][1] in [\"VB\", \"VBD\", \"VBN\", \"VBG\"])):\n",
    "      tag_pattern.append(\"\".join(postag[k][0]) + \" \" + \"\".join(postag[k + 1][0]))\n",
    "      \n",
    "  return tag_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_operator(phrase, word, text):\n",
    "  try:\n",
    "    string = word + r'\\W+(?:\\w+\\W+){0,400}?' + phrase + r'|' + phrase + r'\\W+(?:\\w+\\W+){0,400}?' + word\n",
    "    freq_phrase_near_word = (len(re.findall(string, text)))\n",
    "    return freq_phrase_near_word\n",
    "  except:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Turney(object):\n",
    "  def __init__(self, dataset):\n",
    "    self.datasets = dataset\n",
    "    self.pos_phrases_hits = []\n",
    "    self.neg_phrases_hits = []\n",
    "    self.pos_hits = 0.01\n",
    "    self.neg_hits = 0.01\n",
    "    self.accuracy = 0\n",
    "\n",
    "  def turney(self):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for boolean, test_klass in enumerate(['pos', 'neg']):\n",
    "      for i, data in enumerate(self.datasets['test'][test_klass]):\n",
    "        print(str(i) + \" out of \" + str(len(self.datasets['test'][test_klass])) + \" --> round \" + str(boolean))\n",
    "        phrases = find_pattern(nltk.pos_tag(nltk.word_tokenize(data)))\n",
    "        if len(phrases) == 0:\n",
    "          continue\n",
    "        self.pos_phrases_hits = [0.01] * len(phrases)\n",
    "        self.neg_phrases_hits = [0.01] * len(phrases)\n",
    "        self.pos_hits = 0.01\n",
    "        self.neg_hits = 0.01\n",
    "        for train_klass in ['pos', 'neg']:\n",
    "          for text in self.datasets['train'][train_klass]:\n",
    "            for ind, phrase in enumerate(phrases):\n",
    "              self.pos_phrases_hits[ind] += near_operator(phrase, \"excellent\", text)\n",
    "              self.neg_phrases_hits[ind] += near_operator(phrase, \"poor\", text)\n",
    "              self.pos_hits += text.count(\"excellent\")\n",
    "              self.neg_hits += text.count(\"poor\")\n",
    "        res = self.calculate_sentiment(boolean)\n",
    "        # compute if correct prediction\n",
    "        if res == 1 and boolean == 0:\n",
    "          fp += 1\n",
    "        elif res == 1 and boolean == 1:\n",
    "          tp += 1\n",
    "        elif res == 0 and boolean == 0:\n",
    "          fn += 1\n",
    "        elif res == 0 and boolean == 1:\n",
    "          tn += 1\n",
    "    print(\"Accuracy: \" + str(self.accuracy / 100))\n",
    "    print(\"True positive: \" + str(tp))\n",
    "    print(\"False positive: \" + str(fp))\n",
    "    print(\"True negative: \" + str(tn))\n",
    "    print(\"False negative: \" + str(fn))\n",
    "    print(\"Recall-positive: \" + str(tp / (tp + fn)))\n",
    "    print(\"Precision-positive: \" + str(tp / (tp + fp)))\n",
    "    print(\"Recall-negative: \" + str(tn / (tn + fp)))\n",
    "    print(\"Precision-negative: \" + str(tn / (tn + fn)))\n",
    "\n",
    "  def calculate_sentiment(self, is_negative=0):\n",
    "    polarities = [0] * len(self.pos_phrases_hits)\n",
    "    for i in range(len(self.pos_phrases_hits)):\n",
    "      polarities[i] = math.log((self.pos_phrases_hits[i] * self.neg_hits) / (self.neg_phrases_hits[i] * self.pos_hits), 2)\n",
    "    pmi = sum(polarities) / len(polarities)\n",
    "    if (pmi > 0 and is_negative == 0) or (pmi < 0 and is_negative == 1):\n",
    "      self.accuracy += 1\n",
    "      return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = make_datasets('./datasets/Cell_Phones_and_Accessories_5.json')\n",
    "turney = Turney(datasets)\n",
    "turney.turney()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Supervised Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126415614616154112</td>\n",
       "      <td>Tue Oct 18 21:53:25 +0000 2011</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126404574230740992</td>\n",
       "      <td>Tue Oct 18 21:09:33 +0000 2011</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126402758403305474</td>\n",
       "      <td>Tue Oct 18 21:02:20 +0000 2011</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126397179614068736</td>\n",
       "      <td>Tue Oct 18 20:40:10 +0000 2011</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126395626979196928</td>\n",
       "      <td>Tue Oct 18 20:34:00 +0000 2011</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Sentiment             TweetId                       TweetDate  \\\n",
       "0  apple  positive  126415614616154112  Tue Oct 18 21:53:25 +0000 2011   \n",
       "1  apple  positive  126404574230740992  Tue Oct 18 21:09:33 +0000 2011   \n",
       "2  apple  positive  126402758403305474  Tue Oct 18 21:02:20 +0000 2011   \n",
       "3  apple  positive  126397179614068736  Tue Oct 18 20:40:10 +0000 2011   \n",
       "4  apple  positive  126395626979196928  Tue Oct 18 20:34:00 +0000 2011   \n",
       "\n",
       "                                           TweetText  \n",
       "0  Now all @Apple has to do is get swype on the i...  \n",
       "1  @Apple will be adding more carrier support to ...  \n",
       "2  Hilarious @youtube video - guy does a duet wit...  \n",
       "3  @RIM you made it too easy for me to switch to ...  \n",
       "4  I just realized that the reason I got into twi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA = pd.read_csv('./datasets/full-corpus.csv')\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(DATA['TweetText'], DATA['Sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.93      0.85      0.89       335\n",
      "    negative       0.63      0.67      0.65       115\n",
      "     neutral       0.76      0.84      0.80       464\n",
      "    positive       0.60      0.47      0.53       111\n",
      "\n",
      "    accuracy                           0.78      1025\n",
      "   macro avg       0.73      0.71      0.71      1025\n",
      "weighted avg       0.78      0.78      0.78      1025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "report = classification_report(test_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Text Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jaynakum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jaynakum/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Science and technology often facilitate one another; the latest discoveries in one will lead to new discoveries in the other. Along with innovations in engineering, medicine, and many other fields, this co-evolution can also be seen in physics. The continuing improvements in technology, in particular artificial intelligence (AI) and machine learning (ML), open doors for physics researchers to explore more precise and in-depth topics — leading to new discoveries and a deeper understanding of our world.\n",
    "With roots in statistical mechanics, the mathematical foundation of AI development is shared with many branches of physics, making the two natural counterparts. Since “physics” is an extremely broad subject area and covers many different fields, each field may utilize AI differently.\n",
    "This article will briefly explore the uses of AI in a few different fields of physics, namely particle physics, astrophysics, medical physics, and condensed matter physics.\n",
    "Particle physics dives into the fundamental forces and constituent parts which make up matter and radiation in the universe. In particle physics, artificial intelligence has been implemented to solve many problems. Classification, regression, and anomaly detection are all abilities that AI has that solve problems in particle physics. Artificial intelligence in particle physics is so common that physicists Dr. Matthew Feickert and Dr. Benjamin Nachman have compiled an organized reference page of particle physics research papers and reviews utilizing machine learning.\n",
    "At the Stanford Linear Accelerator Center, run by Stanford University, physicists are using techniques inspired by computer vision to enhance the tagging and processing of images of streams of particles, called jets, produced as a result of the radioactive decay of heavy particles. The same scientists continued to build on their research and used deep learning tools to identify a charge-carrying particle called a W boson. This particle is one of two responsible for the weak force, one of four fundamental forces which govern the interaction of matter in our universe. These processes utilized tools from facial recognition, Monte Carlo simulations, and many more techniques and tools based in artificial intelligence.\n",
    "The European Organization for Nuclear Research, more commonly referred to by the acronym CERN, comes from the French name “Conseil Européen pour la Recherche Nucléaire” and is home to some of the newest and most cutting-edge research in the field of particle physics. One of the most famous machines at CERN is the Large Hadron Collider, also known as the LHC — the world’s largest particle accelerator. Every year, CERN stores over 30 petabytes of data from experiments done with the LHC, the equivalent of 250 years of HD video. With this amount of data, storage would not be possible without filtering techniques, which is one of CERN’s major uses for machine learning. These machine learning techniques can also assist in pattern recognition and determining physical conclusions, which led to the discovery of the Higgs Boson in 2012.\n",
    "Astrophysics covers the physical properties and phenomena behind stellar and astronomical objects in the universe. Similarly to the field of particle physics, astrophysical observations and research produce large quantities of data. The incorporation of AI in astrophysical research is thus heavily centered around data sorting; we can use machine learning algorithms to filter, sort, classify, and identify patterns in data.\n",
    "Research groups in Chile and the United States have begun using a new classification model which aims to identify and classify variable objects directly from images using a deep-learning tool called a recurrent convolutional neural network (RCNN). A team led by Dr. Carrasco-Davis used real-world datasets to train and test the RCNN classification model. This method eliminates certain required steps for previously used classification techniques with images, such as calculating difference images or light curves.\n",
    "Astrophysicists in the Netherlands and Belgium have also trained neural networks for their research, but with a different purpose. Hendriks and Aerts have used deep neural networks to model the cores of intermediate- and high-mass stars during the hydrogen-burning phase. This asteroseismological modelling, which studies vibrations in the matter that makes up the stars, has improved from previous techniques in speed and detail.\n",
    "In the field of medical physics, concepts in physics are applied to diagnosis, treatment, and prevention in health care. This area, like many others in physics, has embraced the development of artificial intelligence over recent years. Using AI in this field can not only improve research, but can also improve procedures and efficiency in healthcare as a whole.\n",
    "At Brown University, researchers have implemented deep learning techniques in their research to enhance identification of blockages in large blood vessels that could lead to strokes. By training neural networks, the group found that the model was sufficient in detecting blockages in large blood vessels and provided results with near-perfect accuracy.\n",
    "Additionally, many research groups are using AI to reduce errors in diagnosis by assisting medical professionals in their decision-making. In 2020, a research group introduced an AI system that could detect earlier stages of breast cancer by studying mammogram images. This detection is done the same way a radiologist would look at the images: the model looks for deviations in the mammogram image compared to images with no cancer. Using a computerized system helps to standardize the procedure and reduce the workload for healthcare professionals. Advancements like this are critical to medical physics and health care in general, as many research centers report low numbers of radiologists. AI assistance can reduce workload and increase the quality and quantity of care medical professionals provide.\n",
    "The field of condensed matter physics explores matter’s large-scale and small-scale properties, most commonly in solid and liquid states. This broad field is highly dominated by quantum mechanics, given the molecular interactions’ scale. Similarly, many modern technologies are also governed by quantum mechanics, as they rely on the fundamental interactions between light and matter and how information is carried on a microscopic level. Naturally, artificial intelligence and condensed matter physics pair well when applied together.\n",
    "Researchers worldwide have begun using AI to assist in the research of quantum materials, an overarching term for material with properties that cannot be explained with classical or semiclassical physics. Using information and data already known about matter and compounds, AI has been able to examine and project the properties of quantum materials. Using the common framework Density Functional Theory, physicists can produce simulations for materials to determine their properties. Machine learning helps to take multidimensional problems and make them more physically understandable.\n",
    "Unlike some other areas of physics, the ties between condensed matter physics and AI can be seen much more clearly, as this field of physics has dramatically impacted the advancements of AI. The theories of condensed matter physics have been applied explicitly to machine learning algorithms through the theories which characterize physical systems on various scales. This theoretical framework, called renormalization group, has been useful for analyzing systems with more than one constituent, referred to as a many-body problem. Through these theories, as well as many others, AI has been trained with more sophisticated algorithms to have then the ability to solve more complex problems.\n",
    "Overall, many advancements in physics would not be possible without the assistance of artificial intelligence. Complex problems require innovative and creative solutions, and the union of physics and artificial intelligence provides the necessary building blocks for uncovering powerful answers. The most difficult situations may still be yet to come, but with continuous breakthroughs in science and technology, researchers will have many tools and techniques at their fingertips.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(document)\n",
    "term_freq = defaultdict(int)\n",
    "for sentence in sentences:\n",
    "  for word in word_tokenize(sentence):\n",
    "    if word.lower() not in stopwords.words('english'):\n",
    "      term_freq[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = defaultdict(int)\n",
    "for sentence in sentences:\n",
    "  for word in word_tokenize(sentence):\n",
    "    if word.lower() in term_freq.keys():\n",
    "      sentence_scores[sentence] += term_freq[word.lower()]\n",
    "sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article will briefly explore the uses of AI in a few different fields of physics, namely particle physics, astrophysics, medical physics, and condensed matter physics. At the Stanford Linear Accelerator Center, run by Stanford University, physicists are using techniques inspired by computer vision to enhance the tagging and processing of images of streams of particles, called jets, produced as a result of the radioactive decay of heavy particles. In the field of medical physics, concepts in physics are applied to diagnosis, treatment, and prevention in health care. Unlike some other areas of physics, the ties between condensed matter physics and AI can be seen much more clearly, as this field of physics has dramatically impacted the advancements of AI. The incorporation of AI in astrophysical research is thus heavily centered around data sorting; we can use machine learning algorithms to filter, sort, classify, and identify patterns in data. Along with innovations in engineering, medicine, and many other fields, this co-evolution can also be seen in physics. The continuing improvements in technology, in particular artificial intelligence (AI) and machine learning (ML), open doors for physics researchers to explore more precise and in-depth topics — leading to new discoveries and a deeper understanding of our world. The European Organization for Nuclear Research, more commonly referred to by the acronym CERN, comes from the French name “Conseil Européen pour la Recherche Nucléaire” and is home to some of the newest and most cutting-edge research in the field of particle physics. This theoretical framework, called renormalization group, has been useful for analyzing systems with more than one constituent, referred to as a many-body problem. With roots in statistical mechanics, the mathematical foundation of AI development is shared with many branches of physics, making the two natural counterparts. Classification, regression, and anomaly detection are all abilities that AI has that solve problems in particle physics. This area, like many others in physics, has embraced the development of artificial intelligence over recent years. With this amount of data, storage would not be possible without filtering techniques, which is one of CERN’s major uses for machine learning. These processes utilized tools from facial recognition, Monte Carlo simulations, and many more techniques and tools based in artificial intelligence. Similarly, many modern technologies are also governed by quantum mechanics, as they rely on the fundamental interactions between light and matter and how information is carried on a microscopic level. Through these theories, as well as many others, AI has been trained with more sophisticated algorithms to have then the ability to solve more complex problems.\n"
     ]
    }
   ],
   "source": [
    "summary_length = int(len(sentences) * 0.3)\n",
    "summary = \" \".join(sorted_sentences[:summary_length])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Multi-document Text Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = ['''Unverified reports of '40 babies beheaded' in Israel-Hamas war inflame social media\n",
    "No photo evidence had been made public as of Thursday morning corroborating claims that babies had been beheaded. Israel has published photos of dead infants after the terror attack.\n",
    "Editor's note: This story includes graphic descriptions of violent acts that some readers may find disturbing.\n",
    "A series of shocking reports have spread horrific claims of baby beheadings by Hamas militants across social and mainstream media in recent days, adding a particularly incendiary element to an already violent and bitter war. But the reports are still unconfirmed, and in some cases have been retracted.\n",
    "The most high-profile claim came Wednesday night when President Joe Biden said that he had seen photographic evidence of terrorists beheading children. The White House later clarified that Biden was referring to news reports about beheadings, which have not included or referred to photographic evidence.\n",
    "Photos have been published by Hamas showing beheaded soldiers and the X account belonging to Israeli Prime Minister Benjamin Netanyahu posted pictures on Thursday of babies killed and burned by Hamas. No photo evidence had been made public as of Thursday morning corroborating claims that babies had been beheaded.\n",
    "Unverified information spreads quickly on social media, particularly around breaking news events, reaching even larger audiences when it is shared by mainstream news outlets, politicians and people with large followings. Follow-ups that retract or add context are less likely to be repeated or reach the same audience.\n",
    "Biden's statement followed a series of news reports and comments from Israeli officials, most of which have since been softened or walked back. Easily debunked misinformation like fake press releases have circulated widely since the start of the war, but such stories often die down quickly once proven false. The claims about beheadings, difficult to verify, have continued to spread thanks in part to the lack of clarity.\n",
    "Alexei Abrahams, a disinformation researcher at McGill University in Montreal, said that even without the allegations of beheaded babies, \"just the facts themselves are horrifying enough to have the kind of effect you expect.\"\n",
    "\"It may turn out that the slaughter was done in a particularly barbaric way. But one way or another, this is an absolutely shocking, unprecedented event of violence,\" Abrahams said. \"The general concern, of course, is that it's going to exacerbate what is already a very fraught situation.\"\n",
    "On Wednesday, a spokesman for Israeli Prime Minister Benjamin Netanyahu told CNN that babies and toddlers were found with their \"heads decapitated\" in southern Israel after Hamas' attack. By Thursday morning, an Israeli official told CNN the government had not confirmed claims of the beheadings.\n",
    "A senior State Department official said Thursday morning that the agency was not in a position to confirm the beheading claims.\n",
    "Many of the reports appear to have originated from Israeli soldiers and people affiliated with the Israel Defense Force (IDF).\n",
    "An IDF spokesperson told Business Insider on Tuesday that soldiers had found decapitated babies, but said Wednesday it would not investigate or provide further evidence regarding the claim. Late Wednesday, an IDF spokesperson said in a video on X that the IDF had \"relative confidence\" of the claims.\n",
    "On Thursday, in a call with a group of international journalists, Colonel Golan Vach, the head of the IDF's national search and rescue unit, said that he had \"found one baby with his head cut.\"\n",
    " Marc Owen Jones, an associate professor of Middle East studies at Hamad Bin Khalifa University in Qatar who studies misinformation, told NBC News that he found that the source of the \"40 babies beheaded\" allegations largely stemmed from a viral Israeli news broadcast clip that did not specifically refer to the allegation.\n",
    "Nicole Zedeck, a correspondent for the privately owned Israeli news outlet i24NEWS, said in the video that Israeli soldiers told her they'd found \"babies, their heads cut off.\" The video has been viewed more than 11 million times on X, according to its view counter. In another tweet, Zedeck wrote that soldiers told her they believe \"40 babies/children were killed.\"\n",
    "\"Somehow those two bits of information were connected, the story became '40 babies were beheaded,' and in the British press today, about six or seven newspapers had it on their front pages,\" Jones said.\n",
    "An IDF spokesperson, Doron Spielman, told NBC News on Tuesday that he could not confirm i24NEWS's report.\n",
    "Yossi Landau, the head of operations for the southern region of Zaka, Israel's volunteer civilian emergency response organization, told CBS News that he saw the bodies of beheaded children and babies, parents and children who had been tortured and had their hands bound, and \"a lot more that cannot be described for now, because it's very hard to describe.\"\n",
    "By Wednesday, the claims, though still contentious, were going viral online — being used as evidence of Hamas' depravity. On Wednesday, the phrase \"Did Hamas kill babies\" saw the biggest increase in search interest on Google of anything related to the war.\n",
    "\"Stranger Things\" star Noah Schnapp posted the shocking claim to his 25 million Instagram followers: \"40 babies were beheaded and burned alive in front of their parents by Hamas.\" Sen. Ted Cruz, R-Texas, mentioned beheaded babies in a post on X, and Rep. Mike McCaul, R-Texas, echoed the allegations on CNN.\n",
    "Jones found that the \"40 babies beheaded\" claim had over 44 million impressions on X, with over 300,000 likes and more than 100,000 reposts. The main accounts propagating the claims were i24NEWS and the official Israel account, Jones' data showed.\n",
    "\"Baby stories are very emotive. Historically, they're stories that can be used to rationalize a very brutal response,\" Jones said. \"It's such a volatile information environment that such claims will inevitably be taken out of context, both deliberately and accidentally.\"'''\n",
    "        ,\n",
    "        '''The 'horrendous toll' on children caught in the Israel-Gaza conflict\n",
    "Hundreds of children have been killed so far, with the true total still unclear.\n",
    "The Israel-Hamas conflict is taking a \"horrendous toll\" on families, humanitarian organizations like UNICEF decried this week, amid reports of the slaughter and kidnapping of children and attacks on civilian infrastructure that have killed, injured or displaced the most vulnerable.\n",
    "In the days since Hamas' surprise assault on Israel, images from both regions have shown crying children running through the street and cowering in bomb shelters after airstrikes. In Gaza, the bodies of dead children killed in shelling were covered in blankets and carried by their fathers in funeral processions. In the kibbutz of Be'eri, one of the largest in Israel, more than 100 bodies of Israeli citizens were discovered on Monday, with women, children and the elderly \"brutally butchered,\" the Israel Defense Forces said. Israeli children have also been among those reported kidnapped by Hamas terrorists.\n",
    "\"Nothing justifies the killing, maiming or abducting of children -- grave rights violations which UNICEF wholeheartedly condemns. Yet less than 72 hours after the outbreak of horrific violence in Israel, reports indicate that grave rights violations against children are rampant,\" UNICEF Executive Director Catherine Russell said in a statement on Monday. \"Many children have been killed or injured, while countless others have been exposed to the violence.\"\n",
    "According to Palestinian authorities, 900 people have been killed in Gaza so far -- including 260 children and 230 women. The number of children killed in Israel is unclear; at least 900 people have died and 2,600 others have been injured, officials said, though did not specify how many were children. Prime Minister Benjamin Netanyahu said in a speech Monday that among the \"atrocities\" committed by Hamas, children have been \"executed with the rest of their families.\"\n",
    "In remarks at the White House Tuesday, President Joe Biden described \"Hamas' bloodthirstiness\" as reminiscent of ISIS rampages -- including \"stomach-churning reports of babies being killed.\"\n",
    "At least 100 civilians and soldiers have also been taken hostage by Hamas militants, Israeli officials said. Hamas leaders on Monday threatened to begin killing hostages one by one and filming the executions if their demands are not met.\n",
    "Among those abducted were 12- and 16-year-old brothers, their mother told ABC News. The woman, who asked not to be identified for security reasons, said the boys were abducted Saturday by Hamas militants who burst into a safe room at their father's home on a kibbutz near the Gaza border.\n",
    "\"I want the world to demand the release of those innocent civilians. I want these children and women and babies back home, and I want my children back home,\" the mother said. \"I can't take a shower without thinking of them being held hostage in some dirty pit somewhere. I can't eat, I can't sleep. I don't think human beings treat people like this. I'm sorry. I want the world to know, to demand those hostages to be returned to their homes.\"\n",
    "In response to the assault, Israel on Monday carried out a \"complete siege,\" cutting off power and blocking food and water from being delivered to the Gaza Strip -- where, according to the CIA, nearly 40% of the population of 2 million is under the age of 15.\n",
    "UNICEF is \"extremely alarmed\" about those measures, spokesperson James Elder said at a press briefing Tuesday in Geneva.\n",
    "\"This will add another layer of suffering to the existing catastrophe faced by families in Gaza,\" Elder said. \"Depriving children of access to food and essential services puts their lives at risk, as do attacks on civilian areas and infrastructure.\"\n",
    "According to UNICEF, 80 of those who live in the Gaza Strip rely on some form of humanitarian assistance.\n",
    "The conflict has led to \"grave humanitarian consequences,\" Lynn Hastings, a humanitarian coordinator for the Gaza Strip for the United Nations Office for the Coordination of Humanitarian Affairs, while calling for all parties to abide by international humanitarian law.\n",
    "\"Civilians, especially children, medical facilities, humanitarian personnel health workers, and journalists must be protected,\" Hastings said in a statement Tuesday. \"Captured civilians must be released immediately and unconditionally.\"\n",
    "UNICEF has also called on all parties to protect children from harm, in accordance with international humanitarian law.\n",
    "\"I remind all parties that in this war, as in all wars, it is children who suffer first and suffer most,\" Russell said.'''\n",
    "        ,\n",
    "        '''What we actually know about the viral report of beheaded babies in Israel\n",
    "One journalist from the Tel Aviv-based news channel i24 said a soldier told her they had \"witnessed… bodies of babies with their heads cut off\" at the Kfar Aza kibbutz near the Gaza border - but no Israeli officials have confirmed the claim.\n",
    "Reports that Israeli soldiers discovered babies that had been beheaded in the Kfar Aza kibbutz are circulating on social and traditional media outlets around the world.\n",
    "The Israel Defence Forces (IDF) invited foreign journalists to see the aftermath of a massacre by Hamas militants at the kibbutz on Tuesday.\n",
    "Sky's chief correspondent Stuart Ramsay was among those to go and see \"stretcher-bearers bringing out a small child\" and a basketball court with \"bodies lined up in black body bags\".\n",
    "But in her TV reports, one journalist from the Tel Aviv-based news channel i24 said a soldier had told her they had \"witnessed… bodies of babies with their heads cut off\".\n",
    "In a statement to Sky News, the IDF said: \"We cannot confirm any numbers. What happened in Kibbutz Kfar Aza is a massacre in which women, children and toddlers and elderly were brutally butchered in an ISIS way of action.\"\n",
    "What happened at the kibbutz?\n",
    "The Kfar Aza kibbutz is one of several self-contained Israeli settlements close to the Gaza border.\n",
    "It is located between Netivot and Sderot - around three miles from the border in southern Israel.\n",
    "Because of its proximity to Gaza and the unprecedented nature of last weekend's incursion, which saw Hamas militants breach the usually heavily guarded border on foot - it was one of the first sites they reached on Saturday.\n",
    "Four days later, journalists got to see the destruction left behind.\n",
    "Ramsay said the scene \"can only be described as a massacre\".\n",
    "\"The stories here are shocking - families being woken without warning to voices outside their houses, mums and dads hiding their children in cupboards, wine cellars and basements, husbands and wives becoming separated in the fight,\" he said.\n",
    "He added it took 17 hours for help to arrive, as the IDF focused on urban areas first - leaving residents defenceless and numbers of dead high.\n",
    "Why are there reports of 'babies being beheaded'?\n",
    "Claims Hamas fighters beheaded babies have only been reported by one journalist - Nicole Zedek from i24 - and have not been verified by Sky News.\n",
    "Ms Zedek was among the reporters invited to see what was left at the kibbutz on Tuesday.\n",
    "In one live broadcast, which has since been viewed millions of times on X, formerly known as Twitter, she says: \"Talking to some of the soldiers here, they say what they witnessed as they've been walking through these communities is bodies of babies with their heads cut off and families gunned down in their beds.\n",
    "\"We can see some of these soldiers right now, comforting each other.\"\n",
    "She is also filmed speaking to the deputy commander of the IDF's unit 71, David Ben Zion, who describes Hamas fighters as \"aggressive\" and \"very bad\".\n",
    "He says: \"They cut off heads… of children, of women.\"\n",
    "And in another live broadcast, Zedek describes \"40 babies at least were taken out on gurneys\" - which is where the widely shared 40 figure comes from.\n",
    "In an interview with Sky's Mark Austin on Tuesday evening, Israeli economy minister Nir Barkat echoed a similar claim: \"We've seen just now... we've heard of 40 young boys. Some of them were burned alive. Some were beheaded. Some were shot in the head.\"\n",
    "CBS News in the US said on Wednesday that Yossi Landau, head of operations at Zaka, Israel's volunteer civilian emergency response organisation, confirmed to them he had \"personally seen\" adults, children and babies beheaded.\n",
    "But when asked directly whether \"40 babies were beheaded\", an IDF spokesman said children were killed - but that reports of beheadings were \"unconfirmed\".\n",
    "It was later reported by at least one major TV news network that the reports of babies being beheaded had been \"confirmed\" by a spokesperson for the Israeli prime minister's office.\n",
    "This was subsequently attributed to Tal Heinrich, a freelance news anchor who appears to have been drafted in by Benjamin Netanyahu's office on 8 October to assist with media relations in the wake of Hamas's attacks a day earlier.\n",
    "The only available public statement on the matter from Ms Heinrich at the time of writing was an interview she conducted with LBC on Wednesday, in which she was asked about the claims.\n",
    "Ms Heinrich, who was quoted by LBC as a spokesperson for Mr Netanyahu's office, said: \"Toddlers, babies, I can tell you some of them... yes, heads were cut off. This is what we are hearing from... soldiers on the ground who dealt with the bodies.\"\n",
    "Replying to a later post on X linking to a story citing her comments, she said on Wednesday evening: \"Please note: We said that these reports are based on testimonies of soldiers.\"\n",
    "'Important to separate facts from speculation'\n",
    "Ramsay interviewed two IDF majors - one of whom was a spokesman.\n",
    "Ramsay said: \"At no point did either he, or the other major I spoke to, ever mention that Hamas had beheaded or killed 40 babies or children. I believe that if it were the case, they would have told me and others there.\n",
    "\"There is no doubt that a horrific attack took place at Kfar Aza, and it needed to be reported, and we did see the bodies of the dead from the community in their houses, in the back of a truck, and on the basketball court.\n",
    "\"But it's important to separate the facts from speculation in a situation like this.\n",
    "\"To reiterate - the IDF had every opportunity to inform the world's media of any story that had become apparent as the military continue to clear up the kibbutz. The murder and beheading of 40 children was never mentioned to me or my team.\"\n",
    "And another journalist, Oren Ziv, who works for independent news outlet 972 mag, was also present and given the opportunity to speak to \"hundreds of soldiers on site\".\n",
    "In a post on X, he said of the baby claims: \"During the tour we didn't see any evidence of this, and the army spokesperson or commanders also didn't mention any such incidents.\"\n",
    "Footage shows how Hamas fighters broke into the kibbutz\n",
    "Adding to the confusion, the White House was forced into a remarkable climbdown on Wednesday night after President Joe Biden appeared to confirm he had seen pictures of children being beheaded in the Hamas attack.\n",
    "In a speech to a Jewish community gathering in Washington, which was televised live, he said: \"I never really thought that I would see, have confirmed, pictures of terrorists beheading children, I never thought I would ever, anyway...\"\n",
    "However, after Sky News' US partner NBC approached the White House for further details on President Biden's remarks, two senior administration officials said Mr Biden was only referring to several media reports from Israel about beheaded children and had not seen images or had independent confirmation of child beheadings.\n",
    "He had not in fact seen any images or had independent confirmation of child beheadings.\n",
    "Digital investigations journalist Victoria Elms, who works on Sky's Data and Forensics unit, adds: \"Social media has been awash with misinformation about the situation in Israel and Gaza since the war broke out.\n",
    "\"Videos from the Syrian conflict, excerpts from video games and TikToks made months ago have all been widely shared, falsely claiming to show events from the past few days.\"\n",
    "She says misinformation is often shared \"unintentionally\", but \"there are some who post and share false material with the intent to deceive others\".\n",
    "\"This is especially dangerous during times of conflict, where it may be even harder than usual to independently verify information or footage,\" she says.\n",
    "\"As the conflict draws on, we would urge users to be vigilant when consuming online content related to the war.'''\n",
    "        ,\n",
    "        '''Israel releases horrific images of slain children after Hamas attack\n",
    "JERUSALEM / TEL AVIV / BRUSSELS - CONTENT WARNING: This story contains graphic details that may not be suitable for all audiences. Reader discretion is advised.\n",
    "Israel's government showed U.S. Secretary of State Antony Blinken and NATO defence ministers graphic images of dead children and civilians on Thursday, saying they were killed by Palestinian group Hamas as it builds support for its response.\n",
    "Prime Minister Benjamin Netanyahu's office also released on social media a picture of a dead infant in a pool of blood and the charred body of a child, part of an apparent effort to stoke global anger against the Gaza militants over Saturday's attack.\n",
    "Blinken, who flew into Tel Aviv earlier on Thursday, told reporters he was shown photographs and videos of a baby riddled with bullets, soldiers beheaded and young people burned alive in their cars or hideaways.\n",
    "\"It's simply depravity in the worst imaginable way,\" Blinken told a news briefing. \"Images are worth a thousand words. These images may be worth a million.\"\n",
    "Netanyahu has vowed to annihilate Hamas following its deadly assault on unsuspecting Israeli communities on Saturday, which killed more than 1,300 people, the deadliest attack on Israel since it was founded in 1948.\n",
    "The Israeli airforce has launched intense bombing raids on Gaza over the past five days and is massing tens of thousands of troops along the border ahead of a possible ground invasion.\n",
    "Gaza authorities said more than 1,400 Palestinians, mainly civilians, including children, have already been killed and more than 6,000 wounded. A land invasion in the densely populated territory could send the toll much higher.\n",
    "Israeli Defence Minister Yoav Gallant played a video to his counterparts at NATO's Brussels headquarters that he said showed horrific scenes from the surprise Hamas attack.\n",
    "\"Children were tied up and shot. Yes, I repeat, children, tied up and shot,\" he told fellow ministers by video link according to a text of his address sent to Reuters.\n",
    "'HORRIFIC PICTURES'\n",
    "In a message on the social media site 'X', Netanyahu's office released what it said were \"horrifying photos of babies murdered and burned by the Hamas monsters.\"\n",
    "It added: \"Hamas is inhuman. Hamas is ISIS,\" comparing the Palestinian group to the Islamic State, which was notorious for its brutality and gory execution videos.\n",
    "The images of the dead infants were included in the video played to NATO. It was not released to the public, but was later seen by Reuters in Jerusalem. Reuters could not independently verify the material.\n",
    "\"They were horrific pictures of the attacks and the victims of the attacks,\" NATO Secretary General Jens Stoltenberg told reporters, saying it \"confirmed the brutality of the attacks.\"\n",
    "The White House said it had no reason to doubt the authenticity of the images.\n",
    "Hamas has denied its militants harmed civilians, accusing Israel and the West of spreading false reports to incite violence against Palestinians.\n",
    "Deputy Hamas chief, Saleh Al-Arouri, said the group's fighters had only aimed to attack the Israeli military and had been surprised by the swift collapse of army units. \"The plan was to target the army's Gaza team and fight occupation soldiers only,\" Arouri said in quotes published by Hamas.\n",
    "The video shown to NATO, apparently taken from a mix of social media published by Hamas and unidentified phone videos, showed the bodies of scores of dead civilians, as well as the body of an Israeli soldier in uniform with his head missing.\n",
    "There were no images to suggest militants had beheaded babies -- a particularly explosive accusation that first emerged in Israel's media and initially confirmed by Israeli officials.\n",
    "U.S. President Joseph Biden had suggested on Wednesday that he had seen images of children beheaded by militants. The White House later clarified that U.S. officials had not seen any evidence of this.\n",
    "Netanyahu has not repeated a claim by his office earlier this week that Hamas had indeed cut off the heads of children, nor did Gallant repeat that accusation to NATO ministers.\n",
    "But medics, international human rights organizations and journalists have documented that militants killed women, children and the elderly as well as young men and soldiers in their rampage.\n",
    "Foreign reporters shown sites targeted by Hamas, witnessed ruins of burnt-out houses and streets scattered with dead residents and militants.\n",
    "NATO officials said they did not expect the alliance to be directly involved in the conflict. But multiple NATO states, above all the United States, have offered Israel military aid.\n",
    "U.S. Defense Secretary Lloyd Austin said after the NATO meeting on Thursday that Washington was not placing any conditions on its security assistance to Israel and expected Israel's professional military to 'do the right things.\\''''\n",
    "        ,\n",
    "        ''''I would see and have confirmed pictures of terrorists beheading children,' Joe Biden decries Hamas atrocity in Israel\n",
    "Reports suggest up to 40 babies slaughtered by Hamas near Gaza Strip. US President Joe Biden expresses horror at beheading of children by terrorists in Israel.\n",
    "Biden spoke to Jewish leaders at the White House on Wednesday and said, \"I never really thought that I would see and have confirmed pictures of terrorists beheading children. I never thought I'd ever — anyway.\" He did not take any questions from the reporters, but said he was working to bring home the Americans who were captured by Hamas and taken to Gaza.\n",
    "\"I haven't given up hope of bringing these folks home,\" he said.\n",
    "\"If I told you, I wouldn't be able to get them home.\"\n",
    "According to reports, up to 40 babies were slaughtered in their homes near the Gaza Strip, which is controlled by Hamas.\n",
    "While a senior White House national security aide stated they hadn't viewed the mentioned images, another White House official cited remarks made by Tal Heinrich, a spokesperson for Israeli Prime Minister Benjamin Netanyahu.\n",
    "On Wednesday, Heinrich quoted CNN that infants and young children had been discovered with \"decapitated\" bodies in the community of Kfar Aza.\n",
    "An Israeli Defense Forces spokesperson also told The Intercept that they could not confirm it officially, but they believed the report.\n",
    "Yossi Landau, a representative from Israel's volunteer civilian emergency response organization, Zaka, shared with CBS News that he witnessed the gruesome sight of children and infants who had been decapitated.\n",
    "\"I saw a lot more that cannot be described for now, because it's very hard to describe,\" he said.\n",
    "Nicole Zedek, a television reporter for i24NEWS based in Tel Aviv, was the initial source to report the allegations of child beheadings on Tuesday. In a radio interview on Wednesday, she expressed her dismay at the initial public skepticism.\n",
    "She stated, \"I mean, babies' heads cut off. That's what they encountered when they came there. So as horrible as it is and and I wish that I it wasn't true.\"\n",
    "The IDF also shared a disturbing image online on Wednesday that showed a blood-stained mattress of an Israeli child with blood spatter on the wall and the footboard.\n",
    "Since Hamas' shocking attack on Israel, the US President has refrained from taking press questions in various settings. He is encountering mounting pressure from members of both parties in Congress to reconsider his decision to release $6 billion to Iran, amid reports that the funds may have been connected to the planning of the attack.'''\n",
    "        ,\n",
    "        '''At least 40 babies killed, beheaded in Israeli kibbutz outside Gaza Strip, reports say\n",
    "KFAR AZA, Israel (TND) — Dozens of babies were reportedly found dead, including some that had been beheaded, in an Israeli kibbutz Tuesday after the terrorist organization Hamas stormed the community.\n",
    "Several journalists were let in to the Kfar Aza kibbutz, located just outside the Gaza Strip, to see the aftermath of the attacks by Hamas. At least 70 residents of the kibbutz were killed by Hamas terrorists, according to Indian news website OpIndia.\n",
    "Nicole Zedeck, a correspondent for Israeli television channel i24NEWS, described the scene as \"truly horrific.\"\n",
    "\"No one could expect that it would be like this, the horrors that I'm hearing from these soldiers,\" Zedeck said. \"As I mentioned earlier, about 40 babies, at least, were taken out on gurneys ... you continue to see just cribs overturned, strollers left behind, all of these doors left wide open.\"\n",
    "Several of the infants were also beheaded by Hamas terrorists, according to OpIndia.\n",
    "Zedeck went on to say that an official death count at the kibbutz is still unknown because soldiers are \"still collecting dead bodies.\"\n",
    "A kibbutz is a small Israeli agricultural community. Kibbutz are dotted throughout Israel, primarily in the Negev Desert.\n",
    "Israel Defense Forces Major General Itai Veruv described the scene in Kfar Aza as a \"massacre\" Tuesday, calling it unlike something Israel has witnessed in \"recent history.\"\n",
    "\"It's not a war, it's not a battlefield, it's a massacre,\" Veruv told The Times of Israel. \"You see the babies, their mothers and their fathers, in their bedrooms, and in their protected rooms, and how the terrorists killed them ... It's something that I never saw in my life.\"\n",
    "The murders at Kfar Aza represent just a fraction of the death and destruction caused by Hamas terrorists. Videos reviewed Monday afternoon show that at least four Israeli citizens were killed shortly after being taken hostage by the terrorist group.\n",
    "An Israeli family of five was reportedly killed by Hamas terrorists during the invasion. The family, which included three children under 7, was discovered dead after the terrorist group infiltrated their bunker.\n",
    "American families have begun to plead with the Biden administration for assistance finding their missing loved ones in Israel. At least 11 U.S. citizens were determined to have been killed in Israel as of Monday.'''\n",
    "        ,\n",
    "        ''''I would see and have confirmed pictures of terrorists beheading children,' Joe Biden decries Hamas atrocity in Israel\n",
    "Reports suggest up to 40 babies slaughtered by Hamas near Gaza Strip. US President Joe Biden expresses horror at beheading of children by terrorists in Israel.\n",
    "Biden spoke to Jewish leaders at the White House on Wednesday and said, \"I never really thought that I would see and have confirmed pictures of terrorists beheading children. I never thought I'd ever — anyway.\" He did not take any questions from the reporters, but said he was working to bring home the Americans who were captured by Hamas and taken to Gaza.\n",
    "\"I haven't given up hope of bringing these folks home,\" he said.\n",
    "\"If I told you, I wouldn't be able to get them home.\"\n",
    "According to reports, up to 40 babies were slaughtered in their homes near the Gaza Strip, which is controlled by Hamas.\n",
    "While a senior White House national security aide stated they hadn't viewed the mentioned images, another White House official cited remarks made by Tal Heinrich, a spokesperson for Israeli Prime Minister Benjamin Netanyahu.\n",
    "On Wednesday, Heinrich quoted CNN that infants and young children had been discovered with \"decapitated\" bodies in the community of Kfar Aza.\n",
    "An Israeli Defense Forces spokesperson also told The Intercept that they could not confirm it officially, but they believed the report.\n",
    "Yossi Landau, a representative from Israel's volunteer civilian emergency response organization, Zaka, shared with CBS News that he witnessed the gruesome sight of children and infants who had been decapitated.\n",
    "\"I saw a lot more that cannot be described for now, because it's very hard to describe,\" he said.\n",
    "Nicole Zedek, a television reporter for i24NEWS based in Tel Aviv, was the initial source to report the allegations of child beheadings on Tuesday. In a radio interview on Wednesday, she expressed her dismay at the initial public skepticism.\n",
    "She stated, \"I mean, babies' heads cut off. That's what they encountered when they came there. So as horrible as it is and and I wish that I it wasn't true.\"\n",
    "The IDF also shared a disturbing image online on Wednesday that showed a blood-stained mattress of an Israeli child with blood spatter on the wall and the footboard.\n",
    "Since Hamas' shocking attack on Israel, the US President has refrained from taking press questions in various settings. He is encountering mounting pressure from members of both parties in Congress to reconsider his decision to release $6 billion to Iran, amid reports that the funds may have been connected to the planning of the attack.'''\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the most representative sentence from each article\n",
    "summaries = []\n",
    "for idx, article in enumerate(DATA):\n",
    "    # Tokenize the article into sentences\n",
    "    sentences = article.split('. ')\n",
    "\n",
    "    # Compute the TF-IDF vectors for the sentences\n",
    "    sentence_vectors = tfidf_vectorizer.transform(sentences)\n",
    "\n",
    "    # Compute cosine similarities between the article and its sentences\n",
    "    cosine_similarities_sentences = linear_kernel(tfidf_matrix[idx:idx+1], sentence_vectors).flatten()\n",
    "\n",
    "    # Find the sentence with the highest cosine similarity\n",
    "    top_sentence_idx = cosine_similarities_sentences.argsort()[-1]\n",
    "    top_sentence = sentences[top_sentence_idx]\n",
    "\n",
    "    summaries.append(top_sentence)\n",
    "merged_summary = '. '.join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector_merged_summary = tfidf_vectorizer.transform([merged_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I never thought I'd ever — anyway.\" He did not take any questions from the reporters, but said he was working to bring home the Americans who were captured by Hamas and taken to Gaza.\n",
      "\"I haven't given up hope of bringing these folks home,\" he said.\n",
      "\"If I told you, I wouldn't be able to get them home.\"\n",
      "According to reports, up to 40 babies were slaughtered in their homes near the Gaza Strip, which is controlled by Hamas.\n",
      "While a senior White House national security aide stated they hadn't viewed the mentioned images, another White House official cited remarks made by Tal Heinrich, a spokesperson for Israeli Prime Minister Benjamin Netanyahu.\n",
      "On Wednesday, Heinrich quoted CNN that infants and young children had been discovered with \"decapitated\" bodies in the community of Kfar Aza.\n",
      "An Israeli Defense Forces spokesperson also told The Intercept that they could not confirm it officially, but they believed the report.\n",
      "Yossi Landau, a representative from Israel's volunteer civilian emergency response organization, Zaka, shared with CBS News that he witnessed the gruesome sight of children and infants who had been decapitated.\n",
      "\"I saw a lot more that cannot be described for now, because it's very hard to describe,\" he said.\n",
      "Nicole Zedek, a television reporter for i24NEWS based in Tel Aviv, was the initial source to report the allegations of child beheadings on Tuesday\n"
     ]
    }
   ],
   "source": [
    "sentences_merged_summary = merged_summary.split('. ')\n",
    "\n",
    "sentence_vectors_merged_summary = tfidf_vectorizer.transform(sentences_merged_summary)\n",
    "\n",
    "cosine_similarities_merged_summary = linear_kernel(tfidf_vector_merged_summary, sentence_vectors_merged_summary).flatten()\n",
    "\n",
    "top_sentence_idx_merged = cosine_similarities_merged_summary.argsort()[-1]\n",
    "top_sentence_merged = sentences_merged_summary[top_sentence_idx_merged]\n",
    "\n",
    "print(top_sentence_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Term Incidence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\"I am a cow\", \"cow is what I am\", \"today is tuesday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'a', 'am', 'cow', 'is', 'today', 'tuesday', 'what']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set([word for doc in DOCS for word in nltk.word_tokenize(doc)])\n",
    "words = list(words)\n",
    "words.sort()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I am a cow': [True, True, True, True, False, False, False, False],\n",
       " 'cow is what I am': [True, False, True, True, True, False, False, True],\n",
       " 'today is tuesday': [False, False, False, False, True, True, True, False]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti_matrix = {}\n",
    "for doc in DOCS:\n",
    "  row = []\n",
    "  for word in words:\n",
    "    if (word in nltk.word_tokenize(doc)):\n",
    "      row.append(True)\n",
    "    else:\n",
    "      row.append(False)\n",
    "  ti_matrix[doc] = row\n",
    "ti_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
