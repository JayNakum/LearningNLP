{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5 - Supervised Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFSKv1MDRBZx",
        "outputId": "98b913a3-0c39-44f2-9d4f-f69660a28b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3897SKzRD8u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the full-corpus.csv dataset\n",
        "full_corpus_df = pd.read_csv(\"/content/drive/MyDrive/NLP/full-corpus.csv\")\n",
        "\n",
        "# Extract the tweets and their sentiments\n",
        "tweets_data = list(zip(full_corpus_df['TweetText'], full_corpus_df['Sentiment']))\n",
        "\n",
        "# Split the data into positive and negative tweets\n",
        "pos_tweets = [(text, 'Positive') for text, sentiment in tweets_data if sentiment.lower() == 'positive']\n",
        "neg_tweets = [(text, 'Negative') for text, sentiment in tweets_data if sentiment.lower() == 'negative']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQfKMrPQSTKF"
      },
      "outputs": [],
      "source": [
        "stopwords = {\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
        "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
        "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
        "    \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
        "    \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
        "    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
        "    \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
        "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
        "    \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\",\n",
        "    \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\", \"here\", \"there\", \"when\",\n",
        "    \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
        "    \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"few\", \"more\", \"most\", \"all\", \"any\", \"none\", \"some\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFHrwNBiUPBK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udAhCqeFUCzt"
      },
      "outputs": [],
      "source": [
        "def preprocess(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)  # Convert URLs to the word URL\n",
        "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet)  # Convert @username to AT_USER\n",
        "    tweet = re.sub('[\\s]+', ' ', tweet)  # Remove additional whitespaces\n",
        "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)  # Replace #word with word\n",
        "    tweet = tweet.strip('\\'\"')  # Trim\n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pTrStf9UI3K"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract word features\n",
        "def get_word_features(wordlist):\n",
        "    wordlist = nltk.FreqDist(wordlist)\n",
        "    word_features = wordlist.keys()\n",
        "    return word_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbZq8JW9UP7d"
      },
      "outputs": [],
      "source": [
        "# Word tokenizer\n",
        "def tokenize(tweet):\n",
        "    return nltk.word_tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf70yaeZUStn"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords and get words from tweets\n",
        "def get_words_in_tweets(tweets):\n",
        "    all_words = []\n",
        "    for (words, sentiment) in tweets:\n",
        "        all_words.extend(tokenize(words))\n",
        "    return all_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7uolSEhUUTL"
      },
      "outputs": [],
      "source": [
        "# Extract features\n",
        "def extract_features(document):\n",
        "    document_words = set(tokenize(document))\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Z9dcEPUWUm"
      },
      "outputs": [],
      "source": [
        "# Replace the hardcoded sample tweets with the extracted tweets\n",
        "tweets = []\n",
        "for (words, sentiment) in pos_tweets + neg_tweets:\n",
        "    tweets.append((preprocess(words), sentiment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm-FtdsgWLg1"
      },
      "outputs": [],
      "source": [
        "# Create word features and train the classifier\n",
        "word_features = get_word_features(get_words_in_tweets(tweets))\n",
        "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCnDZthsUNUT",
        "outputId": "9504f7cb-9967-4d15-a468-4ba50f27c077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Negative\n"
          ]
        }
      ],
      "source": [
        "# Test the classifier (optional, if you want to test with a sample tweet)\n",
        "test_tweet = \"It’s easy to be brave when you’re hiding behind a keyboard. You and your Hamas friends will regret your barbaric actions very soon.\"\n",
        "print(classifier.classify(extract_features(preprocess(test_tweet))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxoCItwyYl6T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# Assuming tweets is a list of (tweet, sentiment) pairs\n",
        "# Splitting data into 80% training and 20% testing\n",
        "train_size = int(len(tweets) * 0.8)\n",
        "train_tweets = tweets[:train_size]\n",
        "test_tweets = tweets[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJIPvMV1Y7wa"
      },
      "outputs": [],
      "source": [
        "# Train the classifier with the training data\n",
        "training_set = nltk.classify.apply_features(extract_features, train_tweets)\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyu5wi5KY4rQ"
      },
      "outputs": [],
      "source": [
        "# Predict the sentiments of the test set\n",
        "predictions = [classifier.classify(extract_features(tweet)) for tweet, _ in test_tweets]\n",
        "actual = [sentiment for _, sentiment in test_tweets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Lo11UJY2_b",
        "outputId": "ba53277e-9e80-4340-8721-b05ce0587bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.1735\n",
            "Precision: 1.0000\n",
            "Recall: 0.1735\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.17      0.30       219\n",
            "    Positive       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.17       219\n",
            "   macro avg       0.50      0.09      0.15       219\n",
            "weighted avg       1.00      0.17      0.30       219\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy, precision, and recall\n",
        "accuracy = accuracy_score(actual, predictions)\n",
        "precision = precision_score(actual, predictions, average='weighted')\n",
        "recall = recall_score(actual, predictions, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# If you want a detailed report for each class (Positive, Negative, etc.)\n",
        "print(classification_report(actual, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pm62LnwZMgf",
        "outputId": "34f23e4d-4ffb-428c-cd52-6706defb193b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F2 Score: 0.2079\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "# Calculate F2 score\n",
        "f2 = fbeta_score(actual, predictions, beta=2, average='weighted')\n",
        "\n",
        "print(f\"F2 Score: {f2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-kc33ReZZH-",
        "outputId": "60f249f5-9ffe-4849-95db-3ae8601f702a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.2957\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(actual, predictions, average='weighted')\n",
        "\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
